{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {Id: 20,\n",
    "    number: \"020\",\n",
    "    title: \"A Basic Q&A LLM model using LangChain\",\n",
    "    info: \"A simple Q&A AI app using Langchain chatmodels and prompt techniques\",\n",
    "    subInfo: \"GROQ, LLM, Langchain Components: Chat Models + Prompt, OpenAI, Ollama\",\n",
    "    imgPath: thb[0],\n",
    "    category: \"cat-c\",\n",
    "    dataSource: \"link\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional and 1st person\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In This project we develop a simple Q&A AI model using lancgchain models and promt techniques. The model used is free source Gemma model throud GROQ interface. It outputs what ever question you ask. In the page I have also include code to access differnet AI model for project and also include code for different prompt techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step1\n",
    "<p>Project intro</p>\n",
    "      <h4></h4>\n",
    "      <Code\n",
    "        code={`\n",
    "          \n",
    "          `}\n",
    "      />\n",
    "      <p>\n",
    "        <br />\n",
    "        <br />\n",
    "      </p>\n",
    "\n",
    "<div className=\"d-block text-center\">\n",
    "        <img\n",
    "          src={img02}\n",
    "          alt=\"result1\"\n",
    "          style={{ height: \"300px\", width: \"300px\" }}\n",
    "        />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# GroQ model - open source\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "\n",
    "# Import various LLMs from LangChain\n",
    "from langchain.llm import OpenAI\n",
    "from langchain.llm import VertexAI\n",
    "from langchain_community.llms import Ollama\n",
    "# Initialize the Ollama model with \"llama3\"\n",
    "llm = Ollama(model=\"llama3\")\n",
    "llm.invoke(\"Tell me a joke\")\n",
    "\n",
    "# Import chat models with appropriate API keys saved in .env file\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Initialize chat models with specified models and API keys\n",
    "chat_openai = ChatOpenAI(model=\"gpt-4\")\n",
    "chat_vertexai = ChatVertexAI(model=\"gemini-pro\")\n",
    "chat_mistralai = ChatMistralAI(model=\"mistral-large-latest\", api_key=\"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompts\n",
    "\n",
    "# Use PromptTemplate to create a template for a string prompt.\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "#output: 'Tell me a funny joke about chickens.'\n",
    "\n",
    "#chat Prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(\n",
    "                                    name=\"Bob\",\n",
    "                                    user_input=\"What is your name?\")\n",
    "#Chat Promt Template using message inputs\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {name}\"),\n",
    "        HumanMessage(content=\"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I am doing well, thanks!\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Message Placeholders\n",
    "from langchain_core.prompts import   ChatPromptTemplate, HumanMessagePromptTemplate,MessagesPlaceholder,\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple Query app using Gemma model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the GroQ model with the API key\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma-7b-It\", groq_api_key=groq_api_key)\n",
    "\n",
    "# Invoke the model to tell a funny joke about fruits\n",
    "model.invoke(\"tell me a funny joke about fruits\")\n",
    "\n",
    "# Import necessary classes for prompting the model\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Create a prompt template to ask about a topic in 10 words\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me little about {topic}\" + \" restrict to 10 words\"\n",
    ")\n",
    "prompt = prompt_template.format(topic='AI')\n",
    "\n",
    "# Create a chat prompt template for a simulated conversation\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        HumanMessage(content=\"Hello, how are you doing?\"),\n",
    "        SystemMessage(content=\"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "chat_prompt = chat_template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")\n",
    "\n",
    "# Invoke the model with the simple prompt\n",
    "response1 = model.invoke(prompt)\n",
    "\n",
    "# Invoke the model with the chat prompt\n",
    "response2 = model.invoke(chat_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "WE begin by loading environment variables from a .env file using the dotenv library, which is essential for securely managing API keys and other sensitive information. The first section initializes the GroQ model, an open-source language model. The API key for GroQ is retrieved from the environment variables, and the ChatGroq class from the langchain_groq module is used to initialize the model with the specified parameters. The model is then invoked to tell a funny joke about fruits.\n",
    "<br/>\n",
    "Next, the code sets up prompting for the model. It imports necessary classes from the langchain_core module and creates a PromptTemplate to generate a prompt that asks the model to provide a brief description of a given topic, restricted to 10 words. This template is then formatted with the topic ‘AI’.\n",
    "<br/>\n",
    "We also creates a ChatPromptTemplate to simulate a chat interaction. It includes system and human messages to guide the conversation. The chat prompt is formatted with the name “Bob” and a user input asking for the bot’s name.\n",
    "<br/>\n",
    "<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
