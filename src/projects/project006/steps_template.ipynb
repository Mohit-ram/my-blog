{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {\n",
    "    'Id':6,\n",
    "    'number':\"006\",\n",
    "    'title': \"Object tracking with yolo\",\n",
    "    'Info': \"Object detection and tracking objects throughout frames only using YOLO\" ,\n",
    "    'subInfo':\"ObjectTrackin ComputerVision OpenCV Yolov8 \",\n",
    "    'imgPath':\"src/projects/project006/mainImg006.jpg\",\n",
    "    'category':cat-a,\n",
    "    'dataSource':\"https://archive.ics.uci.edu/dataset/1/abalone\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1\n",
    "# Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "# Load the YOLO model weights if already downloaded\n",
    "model = YOLO(\"../yolov8n.pt\")\n",
    "# Load the video file\n",
    "cap = cv2.VideoCapture(\"object_tracking_yolo/beach_01.mp4\")\n",
    "# Initialize loop control variable\n",
    "ret = True\n",
    "while ret:\n",
    "    # Read the next frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    # Detect and track objects using YOLO\n",
    "    results = model.track(frame, persist=True)\n",
    "    frame_ = results[0].plot()\n",
    "    # Display the tracked frame\n",
    "    cv2.imshow('video', frame_)\n",
    "    # Exit loop if 'q' key is pressed\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Release video capture resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Importing Libraries and Loading the Model:\n",
    "We begin by importing necessary libraries: ultralytics (for YOLO object detection) and cv2 (OpenCV for video processing). The YOLO class is instantiated with the path to the YOLO model file (\"../yolov8n.pt\") which are already downloaded if not just use YOLO(\"yolov8n.pt\") to download weights to the curent directory.\n",
    "<br>\n",
    "<br>\n",
    "The video file which you want to track object in it (path=\"object_tracking_yolo/sample.mp4\") is loaded using OpenCV’s VideoCapture function. The while loop processes each frame from the video.\n",
    "cap.read() retrieves the next frame (frame) and a boolean value (ret) indicating whether the frame was successfully read.The YOLO model is used to detect objects in the frame using model.track(frame, persist=True).\n",
    "The persist=True argument ensures that the detected objects are tracked across frames.The resulting tracked frame is stored in frame_.\n",
    "<br>\n",
    "<br>\n",
    "The tracked frame (frame_) is now displayed using cv2.imshow('video', frame_). The window will show the video with bounding boxes around detected objects. Pressing the ‘q’ key will exit the loop and close the video window. After the loop completes (or when the user presses ‘q’), the video capture is released (cap.release()) to free up system resources.\n",
    "The OpenCV window is closed using cv2.destroyAllWindows().\n",
    "Optionally, From here, we can also take tracked frames and write them into an output video file using cv2 VideoWritter.\n",
    "<br>\n",
    "<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
