import Code from "../../components/Code/Code.jsx";
import mainImg from "./mainImg002.jpg";

const Content002 = () => {
  return (
    <div className="page container mt-5 mx-auto w-75 px-5 ">
      <h1 className="text-center">Age of Abalones</h1>
      <div className="text-center">
        <img src={mainImg} className="h-50 w-50"></img>
      </div>
      <p>
        The goal of this project is to predict the age of abalones and
        categorising them into: baby, adult or grown; based on their physical
        measurements. Abalones are marine mollusks, and their age is typically
        determined by counting the number of rings in their shells. However,
        this process is time-consuming. We aim to create a regression model that
        predicts the abalone’s age using easily obtainable measurements.
        <br />
        <br />
        The process encompasses loading and visualizing data, transforming
        categorical variables into numerical ones, training a Support Vector
        Classifier, evaluating its performance with cross-validation,
        visualizing the results through a confusion matrix and classification
        report, and finally, saving the model for future use. Each step is
        streamlined to ensure the model is robust, accurate, and ready for
        deployment.
      </p>
      <h4>Data Preprocessing</h4>
      <Code
        code={`
        #Step1: Data preprocessing
        #step1A: Reading data
        import pandas as pd  # Data manipulation
        import numpy as np   # Numerical operations
        # Load and display data
        abalone_data = pd.read_csv('abalone.csv')
        print(abalone_data.info())
        print(abalone_data.head())
        # Prepare features and target
        x = abalone_data.drop('rings', axis=1)
        y = ['baby' if 1 <= r <= 8 else 'adult' if 9 <= r <= 10 else 'grown' for r in abalone_data['rings']]
        y = np.array(y)  # Convert to numpy array
          
          `}
      />
      <p>
        We begin by importing the necessary libraries, pandas and numpy. Then,
        we load the abalone dataset from a CSV file into a DataFrame named
        abalone_data. To understand the structure of our data, we use the
        .info() method, which provides a concise summary of the DataFrame, and
        the .head() method, which displays the first few rows.Next, we prepare
        the feature set x by dropping the ‘rings’ column from abalone_data. The
        ‘rings’ column, which we aim to predict, is excluded from the features
        as it will serve as our target variable.
        <br />
        <br />
        The target variable y is derived from the ‘rings’ column. We categorize
        the abalones into three classes: ‘baby’, ‘adult’, and ‘grown’, based on
        the number of rings. A loop iterates through the ‘rings’ values,
        appending the corresponding class to the list y based on the specified
        conditions.
      </p>

      <Code
        code={`
        #step1B:Visualise and understand data
        import seaborn as sns  # Visualization library
        # Visualizing the 'sex' distribution
        sns.countplot(x='sex', data=abalone_data)
        print("Sex Count in Percentage")
        print(abalone_data.sex.value_counts(normalize=True))
        print("Sex Count in Numbers")
        print(abalone_data.sex.value_counts())
          `}
      />
      <p>
        {" "}
        We use seaborn to visualize the distribution of the ‘sex’ feature in the
        abalone dataset. A count plot provides a quick overview, and we further
        print out the percentage and absolute count of each category.
      </p>
      <div className="container text-center">
      <img
        className="h-50 w-50"
        src="/my-blog/src/projects/project002/sns_countplot.png"
      />
      </div>
      <h4>Data Transformation </h4>
      <Code
        code={`
        #step2: Data transformation:transforming data into numerical
        from sklearn.preprocessing import OneHotEncoder  # Data preprocessing
        from sklearn.compose import ColumnTransformer
        # One-hot encoding the 'sex' feature
        transformer = ColumnTransformer([('encoder', OneHotEncoder(), ['sex'])], remainder='passthrough')
        x_encoded = pd.DataFrame(transformer.fit_transform(x))
          
          `}
      />
      <p>
        The ColumnTransformer is a feature transformer from scikit-learn’s
        compose module. It allows different columns or column subsets of the
        input to be transformed separately and the features generated by each
        transformer will be concatenated to form a single feature space. This is
        particularly useful for heterogeneous data, data that contains a variety
        of data types, as it enables the application of different
        transformations to each type of data.
        <br />
        <br />
        In the above context, we use ColumnTransformer to apply one-hot encoding
        to the ‘sex’ column, which contains categorical data. One-hot encoding
        is a process that converts categorical variables into a form that could
        be provided to machine learning algorithms to do a better job in
        prediction. The ColumnTransformer enables us to integrate this step into
        our preprocessing pipeline, ensuring that the ‘sex’ feature is properly
        encoded before being used to train a model.
      </p>
      <h4>Model Fitting</h4>
      <Code
        code={`
        #step3: model fitting
        from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict
        from sklearn.svm import SVC
        # Splitting the encoded data into training and testing sets
        x_train, x_test, y_train, y_test = train_test_split(x_encoded, y, test_size=0.2, random_state=2)
        # Initializing the Support Vector Classifier with specified parameters
        model = SVC(kernel='rbf', C=1, gamma=100)
        # Fitting the model to the training data
        model.fit(x_train, y_train)
        # Evaluating the model using cross-validation
        scores = cross_validate(model, x_encoded, y, cv=5)
          
          `}
      />
      <p>
        In this step, we fit a Support Vector Classifier (SVC) to the
        preprocessed abalone dataset. The SVC is configured with an RBF kernel,
        regularization parameter C set to 1, and gamma set to 100. We train the
        model using the training data and then evaluate its performance using
        cross-validation with 5 folds.
        
      </p>

      <h4>Results</h4>
      <Code
        code={`
        #step4: visualising results
        from sklearn.metrics import ConfusionMatrixDisplay, classification_report
        # Displaying the confusion matrix using the trained model
        ConfusionMatrixDisplay.from_estimator(model, x_encoded, y)
        # Generating cross-validated predictions for the dataset
        y_pred = cross_val_predict(model, x_encoded, y, cv=5)
        # Displaying the confusion matrix for the predictions
        ConfusionMatrixDisplay.from_predictions(y, y_pred)
        # Printing the classification report for performance metrics
        print(classification_report(y, y_pred))  
        
          
          `}
      />
      <div className="container  d-flex-inline text-center">
        <img className="h-50 w-50" src="/my-blog/src/projects/project002/img01.png"/>
        <img className="h-50 w-50" src="/my-blog/src/projects/project002/img02.png"/>
      </div>
      <p>
        This step involves visualizing the results of the model’s predictions.
        We use a confusion matrix to see how well the model has performed in
        terms of prediction accuracy for each class. Additionally, we generate a
        classification report that provides key metrics such as precision,
        recall, and f1-score for a detailed performance analysis.
      </p>
      <pre>
        precision recall f1-score support adult 0.49 0.49 0.49 1323 baby 0.77
        0.75 0.76 1407 grown 0.65 0.67 0.66 1447 accuracy 0.64 4177 macro avg
        0.64 0.64 0.64 4177 weighted avg 0.64 0.64 0.64 4177
      </pre>
      <Code
        code={`
        #step5: Save and load model
        import joblib  # Library for model persistence
        # Saving the trained model to a file
        joblib.dump(model, "abalone_clf.pkl")
        # Loading the model from the file
        clf = joblib.load("abalone_clf.pkl")
        `}
      />
      <p>
        The fifth step in the workflow is to save the trained model for later
        use, which is known as model persistence. This allows us to reuse the
        model without having to retrain it. We use the joblib library for this
        purpose due to its efficiency with large numpy arrays.
        <br />
        <br />
      </p>
    </div>
  );
};

export default Content002;
