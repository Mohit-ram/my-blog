{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {\n",
    "    'Id':2,\n",
    "    'number:\"002\",\n",
    "    'title': \"Determining the Age of Abalones\",\n",
    "    'Info': \"Appliying of machine learning techniques to the abalone dataset to predict the age category of abalones based on physical measurements.\" ,\n",
    "    'subInfo':\"Classification, Column Transformer, Confusion matrix, F1 score, support vector machines\"\n",
    "    'imgPath':,\n",
    "    'category':cat-a,\n",
    "    'dataSource':\"https://archive.ics.uci.edu/dataset/1/abalone\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The goal of this project is to predict the age of abalones and categorising them into: baby, adult or grown; based on their physical measurements. Abalones are marine mollusks, and their age is typically determined by counting the number of rings in their shells. However, this process is time-consuming. We aim to create a regression model that predicts the abalone’s age using easily obtainable measurements.\n",
    "<br/>\n",
    "<br/>\n",
    "<p>\n",
    "The process encompasses loading and visualizing data, transforming categorical variables into numerical ones, training a Support Vector Classifier, evaluating its performance with cross-validation, visualizing the results through a confusion matrix and classification report, and finally, saving the model for future use. Each step is streamlined to ensure the model is robust, accurate, and ready for deployment.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1: Data preprocessing\n",
    "#step1A: Reading data\n",
    "import pandas as pd  # Data manipulation\n",
    "import numpy as np   # Numerical operations\n",
    "# Load and display data\n",
    "abalone_data = pd.read_csv('abalone.csv')\n",
    "print(abalone_data.info())\n",
    "print(abalone_data.head())\n",
    "# Prepare features and target\n",
    "x = abalone_data.drop('rings', axis=1)\n",
    "y = ['baby' if 1 <= r <= 8 else 'adult' if 9 <= r <= 10 else 'grown' for r in abalone_data['rings']]\n",
    "y = np.array(y)  # Convert to numpy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "We begin by importing the necessary libraries, pandas and numpy. Then, we load the abalone dataset from a CSV file into a DataFrame named abalone_data. To understand the structure of our data, we use the .info() method, which provides a concise summary of the DataFrame, and the .head() method, which displays the first few rows.Next, we prepare the feature set x by dropping the ‘rings’ column from abalone_data. The ‘rings’ column, which we aim to predict, is excluded from the features as it will serve as our target variable. \n",
    "<br/>\n",
    "<br/>\n",
    "The target variable y is derived from the ‘rings’ column. We categorize the abalones into three classes: ‘baby’, ‘adult’, and ‘grown’, based on the number of rings. A loop iterates through the ‘rings’ values, appending the corresponding class to the list y based on the specified conditions.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1B:Visualise and understand data\n",
    "import seaborn as sns  # Visualization library\n",
    "# Visualizing the 'sex' distribution\n",
    "sns.countplot(x='sex', data=abalone_data)\n",
    "print(\"\\nSex Count in Percentage\")\n",
    "print(abalone_data.sex.value_counts(normalize=True))\n",
    "print(\"\\nSex Count in Numbers\")\n",
    "print(abalone_data.sex.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We use seaborn to visualize the distribution of the ‘sex’ feature in the abalone dataset. A count plot provides a quick overview, and we further print out the percentage and absolute count of each category.</p>\n",
    "<img src=\"\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2: Data transformation:transforming data into numerical\n",
    "from sklearn.preprocessing import OneHotEncoder  # Data preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# One-hot encoding the 'sex' feature\n",
    "transformer = ColumnTransformer([('encoder', OneHotEncoder(), ['sex'])], remainder='passthrough')\n",
    "x_encoded = pd.DataFrame(transformer.fit_transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The ColumnTransformer is a feature transformer from scikit-learn’s compose module. It allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is particularly useful for heterogeneous data, data that contains a variety of data types, as it enables the application of different transformations to each type of data.\n",
    "<br/>\n",
    "<br/>\n",
    "In the above context, we use ColumnTransformer to apply one-hot encoding to the ‘sex’ column, which contains categorical data. One-hot encoding is a process that converts categorical variables into a form that could be provided to machine learning algorithms to do a better job in prediction. The ColumnTransformer enables us to integrate this step into our preprocessing pipeline, ensuring that the ‘sex’ feature is properly encoded before being used to train a model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3: model fitting\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "# Splitting the encoded data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_encoded, y, test_size=0.2, random_state=2)\n",
    "# Initializing the Support Vector Classifier with specified parameters\n",
    "model = SVC(kernel='rbf', C=1, gamma=100)\n",
    "# Fitting the model to the training data\n",
    "model.fit(x_train, y_train)\n",
    "# Evaluating the model using cross-validation\n",
    "scores = cross_validate(model, x_encoded, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "In this step, we fit a Support Vector Classifier (SVC) to the preprocessed abalone dataset. The SVC is configured with an RBF kernel, regularization parameter C set to 1, and gamma set to 100. We train the model using the training data and then evaluate its performance using cross-validation with 5 folds.\n",
    "<br/>\n",
    "<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step4: visualising results\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "# Displaying the confusion matrix using the trained model\n",
    "ConfusionMatrixDisplay.from_estimator(model, x_encoded, y)\n",
    "# Generating cross-validated predictions for the dataset\n",
    "y_pred = cross_val_predict(model, x_encoded, y, cv=5)\n",
    "# Displaying the confusion matrix for the predictions\n",
    "ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "# Printing the classification report for performance metrics\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "This step involves visualizing the results of the model’s predictions. We use a confusion matrix to see how well the model has performed in terms of prediction accuracy for each class. Additionally, we generate a classification report that provides key metrics such as precision, recall, and f1-score for a detailed performance analysis.\n",
    "</p>\n",
    "<pre>\n",
    "  precision    recall  f1-score   support\n",
    "       adult       0.49      0.49      0.49      1323\n",
    "        baby       0.77      0.75      0.76      1407\n",
    "       grown       0.65      0.67      0.66      1447\n",
    "    accuracy                           0.64      4177\n",
    "   macro avg       0.64      0.64      0.64      4177\n",
    "weighted avg       0.64      0.64      0.64      4177\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step5: Save and load model\n",
    "import joblib  # Library for model persistence\n",
    "# Saving the trained model to a file\n",
    "joblib.dump(model, \"abalone_clf.pkl\")\n",
    "# Loading the model from the file\n",
    "clf = joblib.load(\"abalone_clf.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The fifth step in the workflow is to save the trained model for later use, which is known as model persistence. This allows us to reuse the model without having to retrain it. We use the joblib library for this purpose due to its efficiency with large numpy arrays.\n",
    "<br/>\n",
    "<br/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
