{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {Id: 23,\n",
    "    number: \"023\",\n",
    "    title: \"Contextual Q&A Chat with AI, End-to-End App\",\n",
    "    info: \"A AI app that answers based on provided context via pdfs and with chat history.\",\n",
    "    subInfo: \"Langchain, History Retreivers,streamlit, HuggingFace, GroQ, Llama\",\n",
    "    imgPath: thb[5],\n",
    "    category: \"cat-d\",\n",
    "    dataSource: \"https://contextualqnaai-k4kibvvy8vspmxwbsx9vfv.streamlit.app/)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional and 1st person\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step1\n",
    "<p>Project intro</p>\n",
    "      <h4></h4>\n",
    "      <Code\n",
    "        code={`\n",
    "          \n",
    "          `}\n",
    "      />\n",
    "      <p>\n",
    "        <br />\n",
    "        <br />\n",
    "      </p>\n",
    "\n",
    "<div className=\"d-block text-center\">\n",
    "        <img\n",
    "          src={img02}\n",
    "          alt=\"result1\"\n",
    "          style={{ height: \"300px\", width: \"300px\" }}\n",
    "        />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1: AI Model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "#Step2: RAG Chain\n",
    "#Data Loadin, Data chunking, Documnet Embedding, VectorStores\n",
    "loader=PyPDFLoader('test.pdf')\n",
    "documents=loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "In the first step, we use the dotenv library to load environment variables from a .env file. This is a secure way to manage sensitive information like API keys. WE then set these environment variables for HuggingFace and Groq API keys. The HuggingFace embeddings model (all-MiniLM-L6-v2) is initialized to convert text into numerical representations (embeddings). The ChatGroq language model (Llama3-8b-8192) is also initialized using the Groq API key.\n",
    "<br/>\n",
    "\n",
    "In the second step, We load a PDF document using the PyPDFLoader class. This document is then split into smaller chunks using the RecursiveCharacterTextSplitter, which ensures that each chunk is of a manageable size (5000 characters) with a slight overlap (200 characters) to maintain context. These chunks are then embedded using the HuggingFace embeddings model and stored in a FAISS (Facebook AI Similarity Search) vector store. This vector store is configured to act as a retriever, allowing efficient retrieval of relevant document chunks based on a query.\n",
    "<br/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prompts and history aware\n",
    "# Chat history aware prompt\n",
    "\n",
    "# System message for reformulating questions based on chat history\n",
    "system_with_history_message = (\n",
    "    \"Given a chat history and the latest user question\"\n",
    "    \" which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a chat prompt template with the system message and placeholders\n",
    "system_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_with_history_message),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever by combining LLM, retriever, and the history-aware prompt\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, system_with_history_prompt)\n",
    "\n",
    "# Actual system prompt for question-answering tasks\n",
    "system_actual_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a chat prompt template for the actual system prompt\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_actual_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "In this step, we create prompts that are aware of the chat history to ensure that questions are contextually accurate and relevant. First, we set up a system message that instructs the model to reformulate a user’s question into a standalone question that can be understood without the chat history. This is achieved using the ChatPromptTemplate with a system message and placeholders for chat history and user input.\n",
    "<br/>\n",
    "Next, we create a history-aware retriever by combining the language model (LLM) and the retriever with the history-aware prompt. This ensures that the retriever can handle context from the chat history effectively.\n",
    "Finally, we set up the actual system prompt for question-answering tasks. This prompt instructs the assistant to use retrieved context to answer questions concisely. The ChatPromptTemplate is used again to structure this prompt with placeholders for chat history and user input.\n",
    "<br/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Runnable chains\n",
    "# Create the question-answer chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create the Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "# Session management\n",
    "store = {}\n",
    "# Function to get session history\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create the conversational RAG chain\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")\n",
    "# Invoke the chain with a sample input and configuration\n",
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what is the topic about?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"chat1\"}\n",
    "    }  \n",
    ")\n",
    "\n",
    "# Retrieve the answer from the response\n",
    "response['answer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'The topic is about the memory of World War I and diverse European narratives about the war.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "In this step, we focus on creating runnable chains to handle the question-answering process effectively. We begin by setting up a question_answer_chain using the create_stuff_documents_chain function. This chain leverages the language model (LLM) and the question-answering prompt (qa_prompt) to generate answers based on the provided context. This ensures that the assistant can produce accurate and relevant responses. Next, we create a rag_chain using the create_retrieval_chain function. This chain integrates the history-aware retriever with the question-answer chain, allowing for contextually relevant answers by considering the entire conversation history. This step is crucial for maintaining the flow and coherence of the conversation, ensuring that the assistant can retrieve and utilize relevant information effectively.\n",
    "<br/>\n",
    "We then manage session histories using a store dictionary. The get_session_history function retrieves the chat history for a given session ID, initializing a new ChatMessageHistory object if the session ID does not exist in the store. This setup allows us to maintain context across different sessions, enhancing the assistant’s ability to provide consistent and context-aware responses.Finally, we create a conversational_rag_chain using the RunnableWithMessageHistory class. This chain combines the RAG chain with the session history retrieval function, specifying keys for input messages, chat history, and output messages. By invoking this chain with a sample input question and a configuration that includes a session ID, we ensure that the assistant can handle conversations with context awareness. This setup allows the assistant to provide accurate and concise answers based on the chat history and retrieved context, enhancing the overall user experience.\n",
    "<br/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
