{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {Id: 19,\n",
    "    number: \"019\",\n",
    "    title: \"NLP: Text Classification\",\n",
    "    info: \"Project demonstrates the application of deep learning techniques to NLP tasks: text classification model.\",\n",
    "    subInfo: \"Text Classification, Tokenisers, BBC news dataset, Embedding Layers\",\n",
    "    imgPath: thb[0],\n",
    "    category: \"cat-c\",\n",
    "    dataSource: \"link\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional and 1st person\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on developing a text classification model using TensorFlow and Keras to categorize BBC News articles into predefined categories. The process involves several key steps, starting with data preprocessing, followed by model definition and training, and concluding with model evaluation. The dataset used is obtained form kaggle competetions at https://www.kaggle.com/c/learn-ai-bbc ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step1\n",
    "<p>Project intro</p>\n",
    "      <h4></h4>\n",
    "      <Code\n",
    "        code={`\n",
    "          \n",
    "          `}\n",
    "      />\n",
    "      <p>\n",
    "        <br />\n",
    "        <br />\n",
    "      </p>\n",
    "\n",
    "<div className=\"d-block text-center\">\n",
    "        <img\n",
    "          src={img02}\n",
    "          alt=\"result1\"\n",
    "          style={{ height: \"300px\", width: \"300px\" }}\n",
    "        />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1: DataPreprocesing\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Data loading\n",
    "train_df = pd.read_csv('BBC News Train.csv', usecols=['Text', 'Category'])\n",
    "test_df = pd.read_csv('BBC News Test.csv', usecols=['Text'])\n",
    "\n",
    "# Data transformation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize tokenizer with a vocabulary size of 1000 and an out-of-vocabulary token\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')\n",
    "def text_to_numeric(text, max_len, num_words=1000):\n",
    "    # Fit tokenizer on the text data\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to ensure uniform length\n",
    "    padded_sequence = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "    return np.array(padded_sequence)\n",
    "\n",
    "# Convert training and testing text data to numerical format\n",
    "train_sequences = text_to_numeric(train_df['Text'], max_len=280)\n",
    "train_labels = text_to_numeric(train_df['Category'], max_len=1) - 1\n",
    "x_test = text_to_numeric(test_df['Text'], max_len=280)\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "split_size = int(0.8 * train_sequences.shape[0])\n",
    "x_train = train_sequences[:split_size]\n",
    "y_train = train_labels[:split_size]\n",
    "x_val = train_sequences[split_size:]\n",
    "y_val = train_labels[split_size:]\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print([p.shape for p in [x_train, x_val, y_train, y_val]])\n",
    "#output: [(1192, 280), (298, 280), (1192, 1), (298, 1)]\n",
    "# Print the tokenizer's word index\n",
    "print(tokenizer.word_index)\n",
    "# output: {'<OOV>': 1, 'sport': 2, 'business': 3, 'politics': 4, 'entertainment': 5, 'tech': 6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "we begin with importing necessary libraries such as TensorFlow, Keras, Pandas, NumPy, and Matplotlib. The data is then loaded from CSV files into Pandas DataFrames, specifically selecting the ‘Text’ and ‘Category’ columns for training data and ‘Text’ for testing data.\n",
    "<br/>\n",
    " The next step involves transforming the text data into numerical format using the Keras Tokenizer. The text_to_numeric function is defined to convert text into sequences of integers, which are then padded to ensure uniform length. This function is applied to both the training and testing datasets. The training labels are also converted to numerical format and adjusted to start from zero.\n",
    " <br/>\n",
    "The data is then split into training and validation sets. The split is done by taking 80% of the data for training and the remaining 20% for validation. Finally, the shapes of the resulting datasets are printed to verify the preprocessing steps. The tokenizer’s word index is also printed to understand the mapping of words to integers\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "inputs = keras.Input(shape=(280,))  # Define input layer with sequence length 280\n",
    "x = keras.layers.Embedding(input_dim=1000, output_dim=16, input_length=280)(inputs)  # Embedding layer\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)  # Global Average Pooling layer\n",
    "x = keras.layers.Dense(24, activation='relu')(x)  # Dense layer with ReLU activation\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(x)  # Output layer with softmax activation\n",
    "model = keras.Model(inputs, outputs)  # Define the model\n",
    "model.summary()  # Print model summary\n",
    "\n",
    "# Model Training\n",
    "model.compile(optimizer=keras.optimizers.Adam(),  # Compile the model with Adam optimizer\n",
    "              loss='sparse_categorical_crossentropy',  # Use sparse categorical cross-entropy loss\n",
    "              metrics=['accuracy'])  # Evaluate model using accuracy metric\n",
    "\n",
    "history = model.fit(x_train, y_train,  # Train the model\n",
    "                    epochs=50, batch_size=16,  # Set number of epochs and batch size\n",
    "                    validation_data=(x_val, y_val))  # Use validation data for evaluation\n",
    "\n",
    "# Plot training and validation loss\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, 'b', label='Training Loss')\n",
    "plt.plot(val_loss, 'bo', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The model architecture begins with an input layer that accepts sequences of length 280. This input is then passed through an embedding layer, which converts the input sequences into dense vectors of fixed size (16 in this case). The embedding layer helps in capturing the semantic meaning of the words.\n",
    "<br/>\n",
    "Following the embedding layer, a Global Average Pooling layer is applied. This layer reduces the dimensionality of the input by averaging the values across the sequence, effectively summarizing the information. Next, a Dense layer with 24 units and ReLU activation function is added. This layer introduces non-linearity to the model, allowing it to learn more complex patterns. The final layer is a Dense layer with 6 units and a softmax activation function, which outputs a probability distribution over the 6 categories.\n",
    "<br/>\n",
    "The model is then compiled with the Adam optimizer, sparse categorical cross-entropy loss function, and accuracy as the evaluation metric. The model is trained on the training data for 50 epochs with a batch size of 16. During training, the model’s performance is also evaluated on the validation set. The training and validation loss values are stored and plotted to visualize the training process.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3:Model Evaluation\n",
    "scores = model.evaluate(x_val, y_val)  # Evaluate the model on validation data\n",
    "print(scores)  #[0.16155174374580383, 0.9563758373260498]\n",
    "\n",
    "# Model Predictions\n",
    "preds = model.predict(x_val)  # Predict the classes for validation data\n",
    "predictions = np.array([np.argmax(pred) for pred in preds])  # Convert predictions to class labels\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted labels\n",
    "preds_df = pd.DataFrame({'actual': y_val.flatten(), 'predictions': predictions})\n",
    "\n",
    "# Calculate accuracy manually\n",
    "accuracy = len(preds_df[preds_df['actual'] == preds_df['predictions']]) / len(y_val)\n",
    "print(accuracy)  #0.9563758389261745"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The model.evaluate function is used to compute the loss and accuracy of the model on the validation data, returning a list of scores. The first element of this list is the loss, and the second element is the accuracy.\n",
    "Next, the model’s predictions on the validation data are obtained using model.predict. These predictions are then converted into class labels by taking the index of the maximum value in each prediction array. A DataFrame is created to compare the actual labels with the predicted labels. Finally, the accuracy of the model is calculated manually by comparing the actual and predicted labels. The proportion of correct predictions is computed by dividing the number of correct predictions by the total number of validation samples.\n",
    "<br/>\n",
    "Acurracy of the model = 0.956 = 96%\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
