{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {Id: 16,\n",
    "    number: \"016\",\n",
    "    title: \"NLP: Sentiment analysis: Sarcasm or not\",\n",
    "    info: \"Sarcasm detection model using deep learning techniques, trained on a dataset of headlines. Streamlit web application for real-time predictions on whether the text is sarcastic.\",\n",
    "    subInfo: \"LSTM, Tokenisatoin, streamlit, sequence data\",\n",
    "    imgPath: thb[16],\n",
    "    category: \"cat-c\",\n",
    "    dataSource: \"link\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional and 1st person\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on developing a machine learning model to detect sarcasm in text using a neural network. The model is trained on a dataset of headlines labeled as sarcastic or not. It involves data preprocessing, model training, and evaluation. Additionally, a user-friendly web application is created using Streamlit, allowing users to input sentences and receive real-time predictions on whether the text is sarcastic. This application leverages the power of deep learning to understand and classify nuanced human language, providing a practical tool for sentiment analysis.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step1\n",
    "<p>Project intro</p>\n",
    "      <h4></h4>\n",
    "      <Code\n",
    "        code={`\n",
    "          \n",
    "          `}\n",
    "      />\n",
    "      <p>\n",
    "        <br />\n",
    "        <br />\n",
    "      </p>\n",
    "\n",
    "<div className=\"d-block text-center\">\n",
    "        <img\n",
    "          src={img02}\n",
    "          alt=\"result1\"\n",
    "          style={{ height: \"300px\", width: \"300px\" }}\n",
    "        />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Loading\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "#run to download sarcasm.json once\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    "# Load data from JSON file\n",
    "with open('sarcasm.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "# Initialize lists for sentences and labels\n",
    "sentences = []\n",
    "labels = []\n",
    "# Extract sentences and labels from the data\n",
    "for row in data:\n",
    "    sentences.append(row['headline'])\n",
    "    labels.append(row['is_sarcastic'])\n",
    "# Step 2: Data Preprocessing\n",
    "# Define parameters for tokenization and padding\n",
    "vocab_size = 1000  # Vocabulary size\n",
    "oov_token = '<OOV>'  # Token for out-of-vocabulary words\n",
    "max_word_length = 60  # Maximum length of sequences\n",
    "padding = 'post'  # Padding type (add padding at the end)\n",
    "trunc_type = 'post'  # Truncation type (truncate at the end)\n",
    "# Initialize the tokenizer\n",
    "tokeniser = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "# Fit the tokenizer on the sentences\n",
    "tokeniser.fit_on_texts(sentences)\n",
    "# Convert sentences to sequences of integers\n",
    "sequences = tokeniser.texts_to_sequences(sentences)\n",
    "# Pad sequences to ensure uniform length\n",
    "padded_seq = pad_sequences(sequences, maxlen=max_word_length, padding=padding, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Step 1: Data Loading\n",
    "\n",
    "The code begins by importing necessary libraries such as TensorFlow, Keras, NumPy, Pandas, and Matplotlib. These libraries are essential for building and training machine learning models, handling data, and visualizing results. The json library is used to load data from a JSON file named sacrasm.json. The data consists of headlines and their corresponding labels indicating whether they are sarcastic or not. The headlines are stored in the sentences list, and the labels are stored in the labels list.\n",
    "\n",
    "Step 2: Data Preprocessing\n",
    "\n",
    "In this step, the text data is converted into a numeric format suitable for machine learning models. The vocabulary size is set to 1000, meaning only the top 1000 most frequent words will be considered. An out-of-vocabulary token (<OOV>) is used to handle words not in the vocabulary. The maximum length of each sequence is set to 60 words, and sequences longer than this will be truncated. The Tokenizer class from Keras is used to tokenize the text data, converting each word into a corresponding integer. These sequences are then padded to ensure they all have the same length, which is necessary for batch processing in neural networks.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3: Data Splits \n",
    "split_size = int(0.7*len(sentences))\n",
    "train_seq = padded_seq[:split_size]\n",
    "val_seq = padded_seq[split_size: split_size+int(0.15*len(sentences))]\n",
    "test_seq = padded_seq[split_size+int(0.15*len(sentences)):]\n",
    "print([val.shape for val in [train_seq, val_seq, test_seq]])\n",
    "train_lab = np.array(labels[:split_size], dtype=np.float32)\n",
    "val_lab = np.array(labels[split_size: split_size+int(0.15*len(sentences))], dtype=np.float32)\n",
    "test_lab = np.array(labels[split_size+int(0.15*len(sentences)):], dtype=np.float32)\n",
    "print([len(val) for val in [train_lab, val_lab, test_lab]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "In step3, the dataset is divided into three subsets: training, validation, and testing. This is a common practice in machine learning to ensure that the model is trained, validated, and tested on different portions of the data, which helps in evaluating the modelâ€™s performance and generalization ability. First, the code calculates the split size for the training set, which is 70% of the total data. The training sequences (train_seq) are then extracted from the padded sequences up to this split size. The validation sequences (val_seq) are taken from the next 15% of the data, and the testing sequences (test_seq) are taken from the remaining 15%.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Architecture\n",
    "# Define the input layer with a fixed sequence length\n",
    "inputs = keras.layers.Input(shape=(max_word_length,))\n",
    "# Add an embedding layer to convert words to dense vectors\n",
    "x = keras.layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=max_word_length)(inputs)\n",
    "# Add a bidirectional LSTM layer to capture dependencies in both directions\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "# Add a dense layer with ReLU activation\n",
    "x = keras.layers.Dense(24, activation='relu')(x)\n",
    "# Add a dropout layer to prevent overfitting\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "# Define the output layer with sigmoid activation for binary classification\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "# Create the model\n",
    "model = keras.Model(inputs, outputs)\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 5: Model Training\n",
    "# Define a callback to save the best model based on validation performance\n",
    "callbacks_list = [keras.callbacks.ModelCheckpoint('sarcasm_model.h5', save_best_only=True)]\n",
    "# Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Train the model on the training data with validation\n",
    "history = model.fit(train_seq, train_lab,\n",
    "                    epochs=10,\n",
    "                    validation_data=(val_seq, val_lab),\n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "# Step 6: Model Conclusion\n",
    "# Plot the training and validation loss\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, 'b')\n",
    "plt.plot(val_loss, 'bo')\n",
    "# Load the best saved model\n",
    "loaded_model = keras.models.load_model('sarcasm_model.h5')\n",
    "# Evaluate the current model on the validation set\n",
    "scores = model.evaluate(val_seq, val_lab)  #[0.39555516839027405, 0.8217673301696777]\n",
    "# Evaluate the loaded model on the validation set\n",
    "new_scores = loaded_model.evaluate(val_seq, val_lab) #[0.3617543876171112, 0.8382426500320435]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Step 4: Model Architecture\n",
    "The model architecture is defined using Keras. It starts with an input layer that accepts sequences of a fixed length. An embedding layer is used to convert these sequences into dense vectors of fixed size. A bidirectional LSTM layer follows, which helps in capturing dependencies in both forward and backward directions. A dense layer with ReLU activation is added, followed by a dropout layer to prevent overfitting. Finally, the output layer uses a sigmoid activation function to produce a probability score for binary classification.\n",
    "<br/>\n",
    "Step 5: Model Training\n",
    "The model is compiled with the Adam optimizer and binary cross-entropy loss function, suitable for binary classification tasks. The training process involves fitting the model on the training data for 10 epochs, with validation on a separate validation set. A callback is used to save the best model based on validation performance.\n",
    "<br/>\n",
    "Step 6: Model Conclusion\n",
    "After training, the loss values for both training and validation sets are plotted to visualize the training process. The best model is loaded, and its performance is evaluated on the validation set. The evaluation scores are printed to compare the performance of the saved model with the current model.\n",
    "<br/>\n",
    "<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#streamlit App\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import json\n",
    "\n",
    "\n",
    "with open('sarcasm.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    sentences = []\n",
    "for row in data:\n",
    "    sentences.append(row['headline'])\n",
    "    \n",
    "\n",
    "def preprocess_data(text):\n",
    "    vocab_size = 1000\n",
    "    oov_token = '<OOV>'\n",
    "    max_word_length = 60\n",
    "    padding='post'\n",
    "    trunc_type = 'post'\n",
    "    #BATCH_SIZE = int(len(sentences)/2)\n",
    "    \n",
    "    tokeniser = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "    tokeniser.fit_on_texts(sentences)\n",
    "    #convert into sequences\n",
    "    sequence = tokeniser.texts_to_sequences(text)\n",
    "    \n",
    "    #pad sequences\n",
    "    padded_seq = pad_sequences(sequence,\n",
    "                                maxlen=max_word_length,\n",
    "                                padding=padding,\n",
    "                            truncating = trunc_type)\n",
    "    \n",
    "    encoded_text = padded_seq\n",
    "    return encoded_text\n",
    "\n",
    "model = load_model('sarcasm_model.h5')\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Sarcastic or not\")\n",
    "st.write(\"Enter a sentence to classify as sarcastic or not.\")\n",
    "\n",
    "user_input = st.text_area(\"Your sentence\")\n",
    "\n",
    "if st.button('yes or no'):\n",
    "    encoded_text = preprocess_data(user_input)\n",
    "    prediction = model.predict(encoded_text)\n",
    "    answer = 'Yes' if prediction[0][0] >0.5 else 'No'\n",
    "    st.write(f'Sarcastic: {answer}')\n",
    "    st.write(f'Prediction Score: {prediction[0][0]}')\n",
    "else:\n",
    "    st.write('Please enter a movie review.')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The main.py below code creates a web application using Streamlit to classify sentences as sarcastic or not. The application loads a pre-trained sarcasm detection model and allows users to input sentences for classification.\n",
    "<br/>\n",
    "Preprocessing Function: A function preprocess_data is defined to preprocess the input text. This function tokenizes the text, converts it into sequences, and pads the sequences to ensure uniform length. The tokenizer is fitted on the sentences from the dataset. The pre-trained sarcasm detection model is loaded using load_model.The Streamlit application is defined and It includes a title and a text area for user input. When the user clicks the button, the input text is preprocessed, and the model predicts whether the sentence is sarcastic or not. The result and prediction score are displayed.\n",
    "<br/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
