{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data Preprocessing\n",
    "#Read the Data\n",
    "# get data from sources and briefly know the data like size, total samples, total feature, feature tpes, any missing values. \n",
    "#load the data into necessary variables and start processing data looking for missing values, simple imputing, distribution of samples.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('iris.csv')\n",
    "data['species'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "#visualise the data for more patterns between features, important features necessary for feature engineering if needed.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "p = sns.pairplot(data=data, hue='species')\n",
    "plt.show()\n",
    "\n",
    "#Ready the data - feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y =np.array(data['species'])\n",
    "x= np.array(data.drop('species', axis=1))\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.15)\n",
    "[v.shape for v in [x_train, x_test, y_train, y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "#Step1A: Read the data\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('iris.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "# Check the distribution of species in the dataset\n",
    "species_distribution = data['species'].value_counts()\n",
    "print(species_distribution)\n",
    "\n",
    "# The output will show the number of samples for each species.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Data Preprocessing - Read and Ready the Data.\n",
    "\n",
    "Reading and Understanding the Data: The first step in data preprocessing is to read the dataset from its source. This is typically done using libraries like pandas, which is a powerful Python library for data analysis. In the provided code, the pd.read_csv('iris.csv') function is used to read the Iris dataset, which is a commonly used dataset in machine learning and statistics. This dataset includes various measurements of iris flowers and their corresponding species.\n",
    "\n",
    "Once the data is loaded into a pandas DataFrame, it’s essential to understand its structure. This involves checking the size of the dataset, the number of samples (rows), the number of features (columns), the types of features (numerical, categorical), and identifying any missing values. The data['species'].value_counts() command counts the number of occurrences of each species in the dataset, providing insight into the distribution of samples.\n",
    "\n",
    "Data Cleaning and Preprocessing: After gaining an initial understanding of the dataset, the next step is to clean and preprocess the data. This involves handling missing values, which can be done through methods such as simple imputation, where missing values are replaced with statistical measures like mean, median, or mode.\n",
    "\n",
    "It’s also important to examine the distribution of samples. If the dataset is imbalanced (i.e., some classes are overrepresented compared to others), it may require resampling techniques to ensure that the model trained on this data does not become biased towards the more frequent classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Data for Pattern Recognition\n",
    "\n",
    "# Import visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pairplot of the dataset\n",
    "# This will plot pairwise relationships in the dataset and color the points by species\n",
    "pair_plot = sns.pairplot(data=data, hue='species')\n",
    "\n",
    "# Display the pairplot\n",
    "plt.show()\n",
    "\n",
    "# The pairplot helps in identifying patterns and relationships between features.\n",
    "# It is also useful for spotting important features for feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1B: Visualizing Data for Pattern Recognition\n",
    "\n",
    "Data visualization is a crucial step in data analysis as it allows for the identification of patterns, trends, and correlations that might not be evident from raw data alone. In the context of the Iris dataset, visualizing the data can help in understanding how the features relate to each other and how they are distributed across different species.\n",
    "\n",
    "The code snippet provided uses the seaborn library, which is built on top of matplotlib and provides a high-level interface for drawing attractive and informative statistical graphics. The sns.pairplot function creates a grid of Axes such that each variable in the dataset will be shared across the y-axes across a single row and the x-axes across a single column. The hue parameter is used to color the data points by the ‘species’ column, which helps in distinguishing the different species from each other.\n",
    "By visualizing the data, one can identify which features are important and how they can be transformed or combined to improve the model’s performance.For instance, if the pairplot shows that certain features separate the species well, those features might be particularly useful for classification tasks. Conversely, if two features are highly correlated, we might consider removing one to reduce redundancy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready the Data for Machine Learning\n",
    "\n",
    "# Import necessary modules from scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the 'species' column into an array and encode it numerically\n",
    "y = np.array(data['species'])\n",
    "y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Drop the 'species' column from the data and convert the rest into an array for feature set\n",
    "x = np.array(data.drop('species', axis=1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# Here, 15% of the data will be used for testing and the rest for training\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.15)\n",
    "\n",
    "# Print the shapes of the resulting arrays to ensure everything is as expected\n",
    "shapes = [v.shape for v in [x_train, x_test, y_train, y_test]]\n",
    "print(shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step1C: Feature Engineering(Data Preparation)\n",
    "Feature engineering is a critical process in machine learning where we transform raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\n",
    "\n",
    "In above code, feature engineering involves encoding categorical variables into a format that can be provided to machine learning algorithms to do a better job in prediction. The LabelEncoder from sklearn.preprocessing is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1. This is useful for converting categorical data, or text data, into numbers, which the machine learning model can understand.\n",
    "The LabelEncoder is applied to the ‘species’ column, which is a categorical feature representing the species of iris flowers. By encoding this column, we convert the species names into a numerical format that can be understood by machine learning algorithms.\n",
    "Species: \n",
    "      [Iris Setosa,Iris Versicolour,Iris Virginica] ->[0,1,2]\n",
    "The train_test_split function is then used to divide the dataset into training and testing sets. This is a common practice in machine learning to evaluate the performance of a model. The test_size=0.15 parameter indicates that 15% of the data will be set aside for testing the model, and the remaining 85% will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Choosing Models and Hyperparameters\n",
    "\n",
    "# Import machine learning models from scikit-learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "HGB_params_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30, None],\n",
    "    'learning_rate': [0.05, 0.1, 0.15]\n",
    "}\n",
    "\n",
    "KNN_params_grid = {\n",
    "    'n_neighbors': range(1, 21, 2),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "SVC_params_grid = {\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "    'C': [50, 10, 1.0, 0.1, 0.01],\n",
    "    'gamma': ['scale']\n",
    "}\n",
    "\n",
    "# Organize models and their hyperparameters in a dictionary\n",
    "models = {\n",
    "    'HGB': {'model': HistGradientBoostingClassifier(), 'params': HGB_params_grid},\n",
    "    'KNN': {'model': KNeighborsClassifier(), 'params': KNN_params_grid},\n",
    "    'SVC': {'model': SVC(), 'params': SVC_params_grid}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: Models selection and hyperparameters\n",
    "In the process of building a machine learning model, selecting the appropriate algorithms and tuning their hyperparameters are crucial steps that can significantly impact the performance of the model.\n",
    "\n",
    "Model Selection: The code snippet outlines the use of three different machine learning models: HistGradientBoostingClassifier, KNeighborsClassifier, and SVC (Support Vector Classifier). Each of these models has its strengths and is suitable for different types of data and problems.\n",
    "\n",
    "HistGradientBoostingClassifier: This is a gradient boosting model for classification tasks which is effective for datasets with a large number of features and instances. It is robust to outliers and can handle missing values natively.\n",
    "KNeighborsClassifier: This model implements the k-nearest neighbors voting algorithm, which is simple yet effective for small to medium-sized datasets. It is a non-parametric method used for classification and regression.\n",
    "SVC: The Support Vector Classifier is a powerful algorithm that works well for both linear and non-linear boundaries. It is particularly useful for datasets where the classes are not linearly separable.\n",
    "Hyperparameter Tuning: Hyperparameters are the configuration settings used to structure the machine learning model. Proper tuning of these parameters can lead to better model accuracy.\n",
    "\n",
    "HGB_params_grid: This dictionary contains hyperparameters for the HistGradientBoostingClassifier, such as max_depth, which determines the maximum depth of the individual estimators, and learning_rate, which affects the contribution of each tree in the ensemble.\n",
    "KNN_params_grid: For the KNeighborsClassifier, the parameters include n_neighbors, which specifies the number of neighbors to use for predictions, weights, which determines the weight function used in prediction, and metric, which defines the distance metric for the tree.\n",
    "SVC_params_grid: The hyperparameters for the SVC include kernel, which specifies the kernel type to be used in the algorithm, C, which is the regularization parameter, and gamma, which defines the influence of a single training example.\n",
    "Models Grid: The models dictionary is a structured way to organize the models and their corresponding hyperparameters. This setup is particularly useful when performing grid search for hyperparameter tuning, as it allows for systematic exploration of the hyperparameter space for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 and Step 4: Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "# Import RandomizedSearchCV for hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a custom evaluation function\n",
    "def custom_eval(models, x_train, y_train, x_test, y_test):\n",
    "    # Initialize dictionaries to store scores and best parameters\n",
    "    models_scores = {}  # Example: 'KNN': { score_metric1: [scores for five CV tests], score_metric2: ... }\n",
    "    model_best_params = {}\n",
    "    \n",
    "    # Iterate over the models and their parameters\n",
    "    for name, model in models.items():\n",
    "        print(name)\n",
    "        print(model['model'])\n",
    "        \n",
    "        # Set up RandomizedSearchCV with the model and its hyperparameter grid\n",
    "        custom_model = RandomizedSearchCV(estimator=model['model'],\n",
    "                                          param_distributions=model['params'],\n",
    "                                          n_iter=5, cv=5, verbose=2, n_jobs=1)\n",
    "        \n",
    "        # Fit the model to the training data\n",
    "        custom_model.fit(x_train, y_train)\n",
    "        \n",
    "        # Store the best parameters and the score of the best model\n",
    "        model_best_params[name] = custom_model.best_params_\n",
    "        models_scores[name] = custom_model.score(x_test, y_test)\n",
    "    \n",
    "    # Return the best parameters and scores for each model\n",
    "    return model_best_params, models_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3 & Step4: Model evaluation and Hypertuning\n",
    "\n",
    "Model Evaluation: Model evaluation is the phase where the performance of different machine learning models is assessed. This is typically done by applying the models to a set of data that they have not seen before, known as the test set. The performance is measured using various metrics, such as accuracy, precision, recall, or F1 score, depending on the nature of the problem.\n",
    "Hyperparameter Tuning: Hyperparameter tuning involves adjusting the parameters of the model that are not learned from the data but are set prior to the training process. These parameters can significantly influence the performance of the model. The RandomizedSearchCV function from scikit-learn is used for hyperparameter tuning. It randomly samples from the given hyperparameter space and performs cross-validation to evaluate the models.\n",
    "Custom Evaluation Function: The custom_eval function defined in the code takes a dictionary of models and their associated hyperparameters, along with training and testing data. It then iterates over each model, performs randomized search cross-validation, and fits the model to the training data. The best hyperparameters for each model are stored, and the performance score of the best model is calculated on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fitting the Best Model from Best Parameters\n",
    "\n",
    "# Retrieve the best parameters and scores from the custom evaluation\n",
    "params, scores = custom_eval(models, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Instantiate the model with the best parameters\n",
    "model = KNeighborsClassifier(weights='distance', n_neighbors=11, metric='minkowski')\n",
    "\n",
    "# Import cross_val_score for cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation to estimate the model's performance\n",
    "cross_validation_scores = cross_val_score(model, x_train, y_train, cv=5)\n",
    "\n",
    "# Calculate the mean of the cross-validation scores\n",
    "average_score = cross_validation_scores.mean()\n",
    "print(f\"Average Cross-Validation Score: {average_score}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "evaluation_score = model.score(x_test, y_test)\n",
    "print(f\"Evaluation Score on Test Data: {evaluation_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step5: Fitting the best Model\n",
    "The final step in the machine learning workflow is to fit the best model identified through hyperparameter tuning to the training data and evaluate its performance. \n",
    "BestModel: After identifying the best hyperparameters for each model using the custom_eval function, the next step is to instantiate the model with these parameters. In the code snippet, the KNeighborsClassifier is chosen with the hyperparameters weights='distance', n_neighbors=11, and metric='minkowski'. These parameters were likely determined to be the best combination during the hyperparameter tuning process.\n",
    "\n",
    "Cross-Validation: Before finalizing the model, it’s common practice to perform cross-validation to estimate its performance. The cross_val_score function is used to assess the model’s accuracy by dividing the training data into cv=5 folds, fitting the model on four folds, and validating it on the fifth fold. This process is repeated five times, each time with a\n",
    "different fold used as the validation set. The score.mean() function calculates the average score across all cross-validation folds, providing an estimate of the model’s performance.\n",
    "\n",
    "Training and Evaluation: Finally, the model is fitted to the entire training dataset using the model.fit(x_train, y_train) method. After training, the model’s performance is evaluated on the test set with the model.score(x_test, y_test) method, which returns the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'clf.pkl')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('clf.pkl')\n",
    "\n",
    "# Predict using the loaded model\n",
    "prediction = loaded_model.predict([x[149]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step6 :Saving and Loading the Model\n",
    "\n",
    "Save the trained KNeighborsClassifier model to a file named ‘clf.pkl’ using joblib.dump.\n",
    "Load the model from ‘clf.pkl’ when needed using joblib.load.\n",
    "Make predictions using the loaded model with predict method."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
