{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {Id: 18,\n",
    "    number: \"018\",\n",
    "    title: \"LangChain: RAG Framework Vector Stores\",\n",
    "    info: \"Creating vectore store DB for custom document using FAIS and Chroma DB\",\n",
    "    subInfo: \"Vector Stores, FIASS, ChromaDB, Ollama Embeddings\",\n",
    "    imgPath: thb[18],\n",
    "    category: \"cat-c\",\n",
    "    dataSource: \"link\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional and 1st person\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on the implementation and utilization of advanced vector stores for document similarity searches. The primary objective is to explore and demonstrate the capabilities of FAISS (Facebook AI Similarity Search) and Chroma vector stores in handling and processing large text datasets. By leveraging these technologies, we aim to enhance the efficiency and accuracy of document retrieval systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step1\n",
    "<p>Project intro</p>\n",
    "      <h4></h4>\n",
    "      <Code\n",
    "        code={`\n",
    "          \n",
    "          `}\n",
    "      />\n",
    "      <p>\n",
    "        <br />\n",
    "        <br />\n",
    "      </p>\n",
    "\n",
    "<div className=\"d-block text-center\">\n",
    "        <img\n",
    "          src={img02}\n",
    "          alt=\"result1\"\n",
    "          style={{ height: \"300px\", width: \"300px\" }}\n",
    "        />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1 Data loading and splitting\n",
    "# Import the necessary classes from the langchain_community and langchain_text_splitters modules\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# Instantiate the TextLoader with the path to the text file\n",
    "loader = TextLoader('sample_story.txt')\n",
    "# Load the contents of the text file into the 'documents' variable\n",
    "documents = loader.load()\n",
    "# Create an instance of CharacterTextSplitter with specified chunk size and overlap\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=30)\n",
    "# Split the loaded documents into smaller chunks and store them in the 'docs' variable\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The above code demonstrates the process of loading and splitting text documents using the langchain_community and langchain_text_splitters libraries. Initially, the TextLoader class from the langchain_community.document_loaders module is imported to facilitate the loading of text files. The CharacterTextSplitter class from the langchain_text_splitters module is also imported to handle the splitting of text into smaller chunks.<br/>\n",
    "The TextLoader is instantiated with the file path 'sample_story.txt', and the load method is called to read the contents of the file into the documents variable. Subsequently, an instance of CharacterTextSplitter is created with specified parameters: chunk_size=100 and chunk_overlap=30. These parameters define the size of each text chunk and the overlap between consecutive chunks, respectively. Finally, the split_documents method is invoked on the text_splitter instance, passing the loaded documents to produce the split text chunks stored in the docs variable.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAISS Vector store\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# Instantiate the OllamaEmbeddings with the specified model\n",
    "embeddings = OllamaEmbeddings(model=\"phi3\")\n",
    "\n",
    "# Define the query string for the similarity search\n",
    "query = \"cave, where a magnificent chest lay waiting. As she opened the \"\n",
    "docs_and_score = db.similarity_search_with_score(query)\n",
    "# Save the FAISS vector store locally with the specified filename\n",
    "db.save_local(\"faiss_db\")\n",
    "# Load the FAISS vector store from the local file with the specified embeddings\n",
    "new_db = FAISS.load_local(\"faiss_Vdb\", embeddings, allow_dangerous_deserialization=True)\n",
    "# Perform a similarity search using the query on the new database object\n",
    "docs = new_db.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The provided code snippet demonstrates the process of creating and utilizing a FAISS (Facebook AI Similarity Search) vector store for document similarity searches. Initially, the OllamaEmbeddings class from the langchain_community.embeddings module is imported and instantiated with the model \"phi3\". This model is used to generate embeddings for the documents. A query string is defined, and the similarity_search_with_score method is called on the db object to find documents similar to the query, along with their similarity scores. The results are stored in the docs_and_score variable. The db object is then saved locally using the save_local method with the filename \"faiss_db\". Subsequently, a new FAISS vector store is loaded from the local file using the FAISS.load_local method.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "# Create a Chroma vector store from docs using the specified embeddings\n",
    "chroma_vdb = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "\n",
    "# Save the entire set of documents to a Chroma vector store on disk in the specified directory\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=\"./chroma_db\")\n",
    "# Load the Chroma vector store from the disk using the specified directory and embeddings\n",
    "db2 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "# Perform a similarity search on the loaded vector store using the specified query\n",
    "docs = db2.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The Chroma.from_documents method is used to create a vector store from the first two documents in the docs list, using the specified embeddings. This vector store is stored in the chroma_vdb variable. Next, the entire set of documents is used to create another Chroma vector store, which is saved to disk in the specified directory ./chroma_db using the persist_directory parameter. This vector store is stored in the vectordb variable. Subsequently, the Chroma vector store is loaded from the disk using the Chroma constructor with the persist_directory parameter set to ./chroma_db and the embedding_function parameter set to the previously defined embeddings. This loaded vector store is stored in the db2 variable.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otput of similarity search.\n",
    "[(Document(metadata={'source': 'sample_story.txt'}, page_content='Upon reaching the island, she discovered a cave entrance hidden behind a waterfall. Inside the cave, she found a series of intricate puzzles and traps designed to protect the treasure. Using her wit and determination, Elara solved each puzzle, narrowly escaping the traps.'),\n",
    "  3823.5085),\n",
    " (Document(metadata={'source': 'sample_story.txt'}, page_content='Elara realized that the true treasure was not the riches, but the journey and the knowledge she had gained. She decided to return to her village and share her newfound wisdom with her people. The journey back was filled with new challenges, but Elara faced them with confidence and courage.'),\n",
    "  4018.107),\n",
    " (Document(metadata={'source': 'sample_story.txt'}, page_content='When she finally returned home, the villagers were amazed by her tales and the knowledge she brought back. Elaraâ€™s adventure inspired others to seek their own paths and explore the world beyond the village. She became a beloved figure, known for her bravery and wisdom.'),\n",
    "  4141.683),\n",
    " (Document(metadata={'source': 'sample_story.txt'}, page_content='Finally, she reached the heart of the cave, where a magnificent chest lay waiting. As she opened the chest, a blinding light filled the cave, revealing a treasure beyond her wildest dreams. But more than gold and jewels, the chest contained ancient scrolls filled with knowledge and wisdom.'),\n",
    "  4170.613)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
