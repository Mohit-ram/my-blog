{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {Id: 15,\n",
    "    number: \"015\",\n",
    "    title: \"NLP: Sentiment prediction using RNN\",\n",
    "    info: \"Build and deploy a sentiment analysis model using the IMDB movie reviews dataset, classifying reviews as positive or negative with a Recurrent Neural Network (RNN).\",\n",
    "    subInfo: \"End-to-End, Deploy, RNN, Embedding vector, PadSequences, Streamlit\",\n",
    "    imgPath: thb[15],\n",
    "    category: \"cat-c\",\n",
    "    dataSource: \"link\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional and 1st person\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on building and deploying a sentiment analysis model using the IMDB movie reviews dataset. Sentiment analysis is a natural language processing (NLP) technique used to determine whether a piece of text is positive, negative, or neutral. In this project, we aim to classify movie reviews as either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step1\n",
    "<p>Project intro</p>\n",
    "      <h4></h4>\n",
    "      <Code\n",
    "        code={`\n",
    "          \n",
    "          `}\n",
    "      />\n",
    "      <p>\n",
    "        <br />\n",
    "        <br />\n",
    "      </p>\n",
    "\n",
    "<div className=\"d-block text-center\">\n",
    "        <img\n",
    "          src={img02}\n",
    "          alt=\"result1\"\n",
    "          style={{ height: \"300px\", width: \"300px\" }}\n",
    "        />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Set the vocabulary size to 5000\n",
    "vocab_size = 5000\n",
    "\n",
    "# Load the IMDB dataset with the specified vocabulary size\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Import the pad_sequences function\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "# Set the maximum length for the input \n",
    "max_length = 250\n",
    "# Pad the training sequences to ensure uniform length\n",
    "x_train_pad = pad_sequences(x_train, max_length)\n",
    "# Pad the test sequences to ensure uniform length\n",
    "x_test_pad = pad_sequences(x_test, max_length)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data preprocesing </h4>\n",
    "<p>\n",
    "The popular IMDB dataset, which is commonly used for sentiment analysis tasks. The preprocessing steps involve loading the dataset, limiting the vocabulary size, and padding the sequences to ensure uniform input length for the neural network.\n",
    "<br/>\n",
    "\n",
    "First, we import the necessary libraries: numpy for numerical operations and tensorflow for building and training the neural network. We then load the IMDB dataset using imdb.load_data(), specifying a vocabulary size of 5000 words. This means that only the top 5000 most frequent words in the dataset will be considered, which helps in reducing the complexity and size of the input data.\n",
    "<br/>\n",
    "\n",
    "Next, we use the pad_sequences function from tensorflow.keras.utils to pad the sequences. Padding ensures that all input sequences have the same length, which is crucial for processing the data in batches. Here, we set the maximum length of the sequences to 250. Any sequence shorter than this length will be padded with zeros, and any sequence longer will be truncated.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module from TensorFlow\n",
    "from tensorflow import keras\n",
    "# Define the input layer with a shape corresponding to the length of the padded sequences\n",
    "inputs = keras.layers.Input(shape=(250,))\n",
    "# Add an embedding layer to transform input integers into dense vectors of size 32\n",
    "x = keras.layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length)(inputs)\n",
    "# Add a SimpleRNN layer with 128 units and ReLU activation function\n",
    "x = keras.layers.SimpleRNN(128, activation='relu')(x)\n",
    "# Add a dense output layer with a sigmoid activation function for binary classification\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "# Create the model by specifying the input and output layers\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "# Print a summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Simple RNN model</h4>\n",
    "<p>\n",
    "In step 2 we construct a simple Recurrent Neural Network (RNN) model using TensorFlow’s Keras API. The model is designed for binary classification tasks, such as sentiment analysis on the IMDB dataset. The architecture includes an embedding layer, a simple RNN layer, and a dense output layer. First, we define the input layer with a shape of (250,), which corresponds to the length of the padded sequences. The embedding layer follows, which transforms the input integers into dense vectors of fixed size (32 in this case). This layer helps the model to learn useful representations of the words in the dataset.\n",
    "<br/>\n",
    "Next, we add a SimpleRNN layer with 128 units and a ReLU activation function. The RNN layer processes the sequence data, capturing temporal dependencies and patterns within the sequences. The output from the RNN layer is then passed to a dense layer with a sigmoid activation function. This dense layer outputs a single value between 0 and 1, suitable for binary classification tasks. Finally, we create the model by specifying the input and output layers and print a summary of the model architecture.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3: Model train and save\n",
    "model.compile(optimizer= 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "history = model.fit(x_train_pad, y_train, epochs=10, batch_size=32, validation_split=0.2 )\n",
    "model.save('rnn_imdb_sentiment.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Model conclusion</h4>\n",
    "<p>\n",
    "The model.compile method configures the model for training by specifying the optimizer, loss function, and evaluation metrics. Here, the Adam optimizer is used for its efficiency, and binary cross-entropy is chosen as the loss function, suitable for binary classification tasks. Accuracy is set as the evaluation metric to monitor the model’s performance.\n",
    "\n",
    "The model.fit method trains the model on the training data (x_train_pad and y_train) for 10 epochs with a batch size of 32. A validation split of 20% is used to evaluate the model’s performance on unseen data during training. Finally, the trained model is saved to a file named ‘rnn_imdb_sentiment.h5’ using the model.save method, allowing for future use without retraining.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StepB: Deploy AI model with streamlit \"app.py\"\n",
    "# Step 1: Import Libraries and Load the Model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "# Load the IMDB dataset word index\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "# Load the pre-trained model with ReLU activation\n",
    "model = load_model('rnn_imdb_sentiment.h5')\n",
    "# Step 2: Helper Functions\n",
    "# Function to decode reviews\n",
    "def decode_review(encoded_review):\n",
    "    # Convert encoded integers back to words\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
    "# Function to preprocess user input\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    # Encode words using the word index\n",
    "    encoded_review = [word_index.get(word, 2) + 3 for word in words]\n",
    "    # Pad the encoded review to a maximum length of 500\n",
    "    padded_review = pad_sequences([encoded_review], maxlen=500)\n",
    "    return padded_review\n",
    "# Step 3: Streamlit Interface\n",
    "# Import Streamlit library\n",
    "import streamlit as st\n",
    "# Streamlit app\n",
    "st.title('IMDB Movie Review Sentiment Analysis')\n",
    "st.write('Enter a movie review to classify it as positive or negative.')\n",
    "# User input\n",
    "user_input = st.text_area('Movie Review')\n",
    "if st.button('Classify'):\n",
    "    # Preprocess the user input\n",
    "    preprocessed_input = preprocess_text(user_input)\n",
    "    # Make prediction\n",
    "    prediction = model.predict(preprocessed_input)\n",
    "    sentiment = 'Positive' if prediction[0][0] > 0.5 else 'Negative'\n",
    "    # Display the result\n",
    "    st.write(f'Sentiment: {sentiment}')\n",
    "    st.write(f'Prediction Score: {prediction[0][0]}')\n",
    "else:\n",
    "    st.write('Please enter a movie review.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "First, we import essential libraries such as numpy for numerical operations, tensorflow for deep learning functionalities, and streamlit for creating the web interface. We then load the IMDB dataset’s word index and the pre-trained model.\n",
    "<br/>\n",
    "We define two helper functions. The decode_review function converts encoded reviews back into readable text using the reverse word index. The preprocess_text function processes user input by converting text to lowercase, splitting it into words, encoding each word using the word index, and padding the sequence to a fixed length.\n",
    "<br/>\n",
    "We set up a Streamlit interface that allows users to input movie reviews and classify them as positive or negative. The app displays a title and a text area for user input. When the “Classify” button is pressed, the user input is preprocessed, and the model predicts the sentiment. The result, either “Positive” or “Negative,” along with the prediction score, is displayed.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
