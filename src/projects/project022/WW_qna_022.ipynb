{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {Id: 22,\n",
    "    number: \"022\",\n",
    "    title: \"Langchain: Deployed World Wars Q&A AI, end-to-end\",\n",
    "    info: \"End to End AI app deployment using langchain, Groq, Huggingface, streamlit\",\n",
    "    subInfo: \"Langchain, streamlit, HuggingFace, GroQ, Llama\",\n",
    "    imgPath: thb[5],\n",
    "    category: \"cat-d\",\n",
    "    dataSource: \"link\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional and 1st person\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aim to build a sophisticated Retrieval-Augmented Generation (RAG) system using various tools and technologies. The project involves several key steps, from setting up the language model to deploying the application. The entire system is integrated with Streamlit, leveraging its Session State feature to maintain and share variables between reruns, ensuring a seamless user experience. Finally, the Streamlit application is deployed to the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step1\n",
    "<p>Project intro</p>\n",
    "      <h4></h4>\n",
    "      <Code\n",
    "        code={`\n",
    "          \n",
    "          `}\n",
    "      />\n",
    "      <p>\n",
    "        <br />\n",
    "        <br />\n",
    "      </p>\n",
    "\n",
    "<div className=\"d-block text-center\">\n",
    "        <img\n",
    "          src={img02}\n",
    "          alt=\"result1\"\n",
    "          style={{ height: \"300px\", width: \"300px\" }}\n",
    "        />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1: LLM model\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN']=st.secrets['HF_TOKEN']\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "llm_model = ChatGroq(model='Llama3-8b-8192',groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "we import the ChatGroq class from the langchain_groq module. This class is likely used to interact with a language model provided by Groq. The load_dotenv() function is called to load environment variables from a .env file, which is a common practice to manage sensitive information like API keys and tokens. The os.environ['HF_TOKEN'] line sets the Hugging Face token from the streamlit secrets, ensuring secure access to the Hugging Face API.\n",
    "<br/>\n",
    "Next, the code retrieves the Groq API key from the environment variables using os.getenv('GROQ_API_KEY'). This key is essential for authenticating requests to the Groq API. Finally, the llm_model variable is initialized with an instance of ChatGroq, specifying the model name Llama3-8b-8192 and the Groq API key. This setup prepares the language model for further use in the application.\n",
    "<br/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1: LLM model\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN']=st.secrets['HF_TOKEN']\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "llm_model = ChatGroq(model='Llama3-8b-8192',groq_api_key=groq_api_key)\n",
    "\n",
    "#Step2: RAG Implementation\n",
    "#Data injection\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "loader=PyPDFDirectoryLoader(\"wars_pdf\")\n",
    "docs=loader.load()\n",
    "#Data chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "final_documents=text_splitter.split_documents(docs)\n",
    "#Documnet Embedding\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#FAISS Vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store=FAISS.from_documents(final_documents,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "we import the PyPDFDirectoryLoader class from the langchain_community.document_loaders module. This class is used to load PDF documents from a specified directory, in this case, “wars_pdf”. The loaded documents are stored in the docs variable. Next, we use the RecursiveCharacterTextSplitter from the langchain.text_splitter module to split the loaded documents into smaller chunks. This is done to manage large documents more effectively. The chunk_size is set to 1000 characters with an overlap of 100 characters, ensuring that the chunks are manageable and that important information isn’t lost between chunks. The resulting chunks are stored in the final_documents variable.\n",
    "<br/>\n",
    "We then imports the HuggingFaceEmbeddings class from the langchain_huggingface module to create embeddings for the document chunks. The model used for this purpose is “all-MiniLM-L6-v2”, which is known for its efficiency in generating embeddings. Finally, we import the FAISS class from the langchain_community.vectorstores module to create a vector store from the document embeddings. This vector store is used to efficiently search and retrieve relevant document chunks based on their embeddings.\n",
    "<br/>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step:Prompts|Chains|retreival\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the questions based on the provided context only.\n",
    "    Please provide the most accurate respone based on the question\n",
    "    <context>\n",
    "    {context}\n",
    "    <context>\n",
    "    Question:{input}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "document_chain=create_stuff_documents_chain(llm_model,prompt)\n",
    "retriever=vector_store.as_retriever()\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "response=retrieval_chain.invoke({'input':\"How many countries involved in world war 1 and their names\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Based on the provided context, the following countries are mentioned as being involved in World War I:\\n\\n1. Germany\\n2. Austria\\n3. Hungary\\n4. Turkey\\n5. Poland\\n6. Czech Republic\\n7. Slovakia\\n8. Ukraine\\n9. Britain\\n10. France\\n11. United States\\n12. Soviet Union\\n13. Italy\\n14. Japan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "We import the ChatPromptTemplate class from the langchain_core.prompts module. This class is used to create a prompt template for the language model. The prompt variable is initialized with a template that instructs the model to answer questions based on the provided context only, ensuring accurate responses. Next, the code imports functions to create document and retrieval chains. The create_stuff_documents_chain function is used to create a chain that combines documents using the language model and the prompt template. This chain is stored in the document_chain variable. The vector_store is converted into a retriever using the as_retriever method. This retriever is then used to create a retrieval chain with the create_retrieval_chain function, which combines the retriever and the document chain. The resulting retrieval chain is stored in the retrieval_chain variable.\n",
    "\n",
    "<br/>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step4: Streamlit interface complete app\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN']=st.secrets['HF_TOKEN']\n",
    "groq_api_key = st.secrets['GROQ_API_KEY']\n",
    "\n",
    "#Step1: LLM model\n",
    "\n",
    "llm_model = ChatGroq(model='Llama3-8b-8192',groq_api_key=groq_api_key)\n",
    "#llm_model.invoke([HumanMessage(content=\"tell me joke\")])\n",
    "\n",
    "#Step2: RAG implementation\n",
    "# Data injection and chunking\n",
    "#Document Embedding and Vectore stores\n",
    "def create_vectore_store():\n",
    "    if \"vectore_store\" not in st.session_state:\n",
    "        st.session_state.embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        st.session_state.loader = PyPDFDirectoryLoader(\"wars_pdf\")\n",
    "        st.session_state.docs=st.session_state.loader.load()\n",
    "        st.session_state.text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "        st.session_state.final_documents=st.session_state.text_splitter.split_documents(st.session_state.docs)\n",
    "        st.session_state.vectors=FAISS.from_documents(st.session_state.final_documents,st.session_state.embeddings)\n",
    "\n",
    "st.title(\"World wars Q&A with Llama3 based AI\")\n",
    "st.write(\"First Press start and wait for AI to be trainied\")\n",
    "st.write(\"Trainied on only small data of world wars.\")\n",
    "user_prompt = st.text_input(\"Enter question regarding world wars\")\n",
    "\n",
    "if st.button(\"Start\"):\n",
    "    create_vectore_store()\n",
    "    st.write(\"Trainied and ready, please enter questions!\")\n",
    "\n",
    "#Step3: Chains and Retrevials\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the questions based on the provided context only.\n",
    "    Please provide the most accurate respone based on the question\n",
    "    <context>\n",
    "    {context}\n",
    "    <context>\n",
    "    Question:{input}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "\n",
    "if user_prompt:\n",
    "    document_chain=create_stuff_documents_chain(llm_model,prompt)\n",
    "    retriever=st.session_state.vectors.as_retriever()\n",
    "    retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "\n",
    "    response = retrieval_chain.invoke({'input': user_prompt})\n",
    "\n",
    "    st.write(response['answer'])\n",
    "\n",
    "    with st.expander(\"Document similarity Search\"):\n",
    "        for i,doc in enumerate(response['context']):\n",
    "            st.write(doc.page_content)\n",
    "            st.write('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "1. st.session_state: Streamlit’s Session State is a feature that allows you to maintain and share variables between reruns of your Streamlit app, ensuring a more interactive and dynamic user experience. Each time a user interacts with your app, Streamlit reruns the script from top to bottom. Without Session State, variables would reset with each rerun, losing any user-specific data.Session State enables you to store and persist data across these reruns for each user session. In our case we implement al the RAG frame work only once at the start and we use same vector store for all refreshs of page, which in turn saves time.\n",
    "2.user_promt: We get question from user as user_promt variable and create chains prompts and retreivers accordingly and output the appropriate result. \n",
    "<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5: Deployment in Sreamlit cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Deployment in Sreamlit cloud</h4>\n",
    "<br/>1. Push current directory files (app.py, requirements, pdf_folder) to a new git repository\n",
    "<br/>2. Commit changes and loign streamlit cloud and connect with github.\n",
    "<br/>2. Go to Streamlit cloud then \"create app\" select git repo, under advanced setting paste GROQ api key as secret key (groq_api_key=\"gq-ddsa...\")\n",
    "<br/>3. save and press deploy. and wait until app is build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
