{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prj js object\n",
    "js object\n",
    "const prj = {Id: 7,\n",
    "    number: \"007\",\n",
    "    title: \"Wildlife Detection with YOLOv8 and Ultralytics\",\n",
    "    info: \"The project demonstrates object detection using YOLOv8 and the Ultralytics library. Fine-tune the model on an “african-wildlife” dataset and perform inference on sample images.\",\n",
    "    subInfo: \"Obeject Detection, Google Colab, Yolov8, Ultralytics Dataset, Yolo metrics, mAP50.\"\n",
    "    imgPath: thb[7],\n",
    "    category: \"cat-c\",\n",
    "    dataSource: \"https://docs.ultralytics.com/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: explain code into paragraphs briefly and at last provide code with proper comments.\n",
    "Style: Academic\n",
    "Tone: Professional\n",
    "Audience: 30-year old\n",
    "Format: Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1\n",
    "!nvidia-smi\n",
    "!pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "# Train the model\n",
    "results = model.train(data=\"african-wildlife.yaml\", epochs=30, imgsz=640)\n",
    "# Load a model\n",
    "model = YOLO(\"/content/runs/detect/train/weights/best.pt\")  # load a brain-tumor fine-tuned model\n",
    "# Inference using the model\n",
    "results = model.predict(\"/content/elephants-1900332_1280.jpg\", save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/african-wildlife.yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image 1/1 /content/elephants-1900332_1280.jpg: 384x640 2 buffalos, 3 elephants, 102.1ms\n",
    "Speed: 1.9ms preprocess, 102.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
    "Results saved to runs/detect/predict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "We begin by importing necessary libraries and modules. Specifically, import the YOLO class from the ultralytics package. The YOLO (You Only Look Once) algorithm is a popular object detection model that can identify and locate multiple objects within an image. Next, we load a pre-trained YOLO model using the file \"yolov8n.pt\". Pre-trained models are recommended for training because they have already learned useful features from a large dataset. The model object now contains the pre-trained YOLO model.\n",
    "<br/>\n",
    "<br/>\n",
    "Now, we proceeds to train the YOLO model using the specified parameters: data=\"african-wildlife.yaml\": This points to a YAML (https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/african-wildlife.yaml.) file containing information about the dataset used for training. The dataset contains labeled images of African wildlife provide by ultralytics. epochs=30: The model will be trained for 30 epochs (iterations over the entire dataset). imgsz=640: The input image size during training is set to 640x640 pixels.\n",
    "<br/>\n",
    "<br/>\n",
    "Training involves adjusting the model’s weights based on the dataset to improve its ability to detect objects accurately. After training, yolo result contain various prameters and perfomance metrics to evaluate the model. The best model weight are also save as best.pt in detect/train/weights. We load the best weights by calling YOLO on it. Finally, we perform the inference (object detection) using the loaded model. It predicts objects in an image specified by the file path\n",
    "The save=True parameter indicates that the results (bounding boxes, class labels, confidence scores) should be saved in to an image.\n",
    "<br/>\n",
    "<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Yolo Metrics:\n",
    "Box Loss (box_loss):The box loss measures the discrepancy between predicted bounding boxes and ground truth bounding boxes.\n",
    "It penalizes incorrect localization (i.e., inaccurate bounding box coordinates). Lower box loss indicates better localization accuracy.\n",
    "<br/>\n",
    "<br/>\n",
    "Class Loss (cls_loss):The class loss evaluates the correctness of predicted class labels. It penalizes misclassifications (e.g., predicting “car” when the true label is “person”). Lower class loss indicates better classification performance.\n",
    "<br/>\n",
    "<br/>\n",
    "DFL Loss (dfl_loss): DFL (Distribution Focal Loss) is an improvement over the standard focal loss. It addresses class imbalance by emphasizing hard-to-classify examples. DFL loss combines both localization and classification aspects.\n",
    "<br/>\n",
    "<br/>\n",
    "mAP50 (Mean Average Precision @ IoU 0.5):\n",
    "mAP50 evaluates object detection accuracy across different object classes.\n",
    "It computes the average precision (AP) at IoU (Intersection over Union) threshold of 0.5. Higher mAP50 indicates better overall performance.\n",
    "<br/>\n",
    "<br/>\n",
    "mAP50-90 (Mean Average Precision @ IoU 0.5-0.9): mAP50-90 considers a range of IoU thresholds (from 0.5 to 0.9). It provides a more comprehensive assessment of detection quality. Higher mAP50-90 reflects better performance across various IoU levels.\n",
    "<br/>\n",
    "<br/>\n",
    "Recall (R): Recall measures the proportion of true positive detections out of all actual positive instances. It indicates how well the model captures relevant objects. Higher recall means fewer missed detections.\n",
    "Precision (P): Precision calculates the proportion of true positive detections out of all predicted positive instances. It assesses the model’s ability to avoid false positives. Higher precision means fewer false alarms.\n",
    "<br/>\n",
    "<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Precision-Recall Curve:\n",
    "The precision-recall curve is a graphical representation that helps us understand the trade-off between precision and recall for different confidence thresholds in object detection.\n",
    "Precision: Measures the accuracy of positive predictions. It’s the ratio of true positives to the total number of positive predictions (true positives + false positives).\n",
    "Recall: Also known as sensitivity or true positive rate, it measures the proportion of actual positive instances correctly detected by the model.\n",
    "The curve shows how precision and recall change as we vary the confidence threshold. A higher threshold leads to higher precision but lower recall, while a lower threshold increases recall but may reduce precision1.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
